{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import contextlib\n",
    "import functools\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from six.moves import urllib\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = os.path.join(os.sep, 'tmp', 'datasets')\n",
    "\n",
    "\n",
    "def make_val_and_grad_fn(value_fn):\n",
    "  @functools.wraps(value_fn)\n",
    "  def val_and_grad(x):\n",
    "    return tfp.math.value_and_gradient(value_fn, x)\n",
    "  return val_and_grad\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def timed_execution():\n",
    "  t0 = time.time()\n",
    "  yield\n",
    "  dt = time.time() - t0\n",
    "  print('Evaluation took: %f seconds' % dt)\n",
    "\n",
    "\n",
    "def np_value(tensor):\n",
    "  \"\"\"Get numpy value out of possibly nested tuple of tensors.\"\"\"\n",
    "  if isinstance(tensor, tuple):\n",
    "    return type(tensor)(*(np_value(t) for t in tensor))\n",
    "  else:\n",
    "    return tensor.numpy()\n",
    "\n",
    "def run(optimizer):\n",
    "  \"\"\"Run an optimizer and measure it's evaluation time.\"\"\"\n",
    "  optimizer()  # Warmup.\n",
    "  with timed_execution():\n",
    "    result = optimizer()\n",
    "  return np_value(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example from the book \"The Elements of Statistical Learning, Data Mining, Inference, and Prediction\" by Trevor Hastie, Robert Tibshirani, and Jerome Friedman.\n",
    "\n",
    "Note that this is an optimization problem using the L1 penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_or_download_file(cache_dir, url_base, filename):\n",
    "  \"\"\"Read a cached file or download it.\"\"\"\n",
    "  filepath = os.path.join(cache_dir, filename)\n",
    "  if tf.io.gfile.exists(filepath):\n",
    "    return filepath\n",
    "  if not tf.io.gfile.exists(cache_dir):\n",
    "    tf.io.gfile.makedirs(cache_dir)\n",
    "  url = url_base + filename\n",
    "  print(\"Downloading {url} to {filepath}.\".format(url=url, filepath=filepath))\n",
    "  urllib.request.urlretrieve(url, filepath)\n",
    "  return filepath\n",
    "\n",
    "def get_prostate_dataset(cache_dir=CACHE_DIR):\n",
    "  \"\"\"Download the prostate dataset and read as Pandas dataframe.\"\"\"\n",
    "  url_base = 'http://web.stanford.edu/~hastie/ElemStatLearn/datasets/'\n",
    "  return pd.read_csv(\n",
    "      cache_or_download_file(cache_dir, url_base, 'prostate.data'),\n",
    "      delim_whitespace=True, index_col=0)\n",
    "\n",
    "prostate_df = get_prostate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lcavol</th>\n",
       "      <th>lweight</th>\n",
       "      <th>age</th>\n",
       "      <th>lbph</th>\n",
       "      <th>svi</th>\n",
       "      <th>lcp</th>\n",
       "      <th>gleason</th>\n",
       "      <th>pgg45</th>\n",
       "      <th>lpsa</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.579818</td>\n",
       "      <td>2.769459</td>\n",
       "      <td>50</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.430783</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.994252</td>\n",
       "      <td>3.319626</td>\n",
       "      <td>58</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.162519</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.510826</td>\n",
       "      <td>2.691243</td>\n",
       "      <td>74</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.162519</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.203973</td>\n",
       "      <td>3.282789</td>\n",
       "      <td>58</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.162519</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.751416</td>\n",
       "      <td>3.432373</td>\n",
       "      <td>62</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.371564</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.049822</td>\n",
       "      <td>3.228826</td>\n",
       "      <td>50</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.765468</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.737164</td>\n",
       "      <td>3.473518</td>\n",
       "      <td>64</td>\n",
       "      <td>0.615186</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.765468</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.693147</td>\n",
       "      <td>3.539509</td>\n",
       "      <td>58</td>\n",
       "      <td>1.536867</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.854415</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.776529</td>\n",
       "      <td>3.539509</td>\n",
       "      <td>47</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.047319</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.223144</td>\n",
       "      <td>3.244544</td>\n",
       "      <td>63</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.047319</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.254642</td>\n",
       "      <td>3.604138</td>\n",
       "      <td>65</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.266948</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.347074</td>\n",
       "      <td>3.598681</td>\n",
       "      <td>63</td>\n",
       "      <td>1.266948</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.266948</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.613430</td>\n",
       "      <td>3.022861</td>\n",
       "      <td>63</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.597837</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>1.266948</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.477049</td>\n",
       "      <td>2.998229</td>\n",
       "      <td>67</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1.348073</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.205971</td>\n",
       "      <td>3.442019</td>\n",
       "      <td>57</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.430783</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1.398717</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.541159</td>\n",
       "      <td>3.061052</td>\n",
       "      <td>66</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.446919</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.415515</td>\n",
       "      <td>3.516013</td>\n",
       "      <td>70</td>\n",
       "      <td>1.244155</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.597837</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>1.470176</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.288486</td>\n",
       "      <td>3.649359</td>\n",
       "      <td>66</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>0.371564</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.492904</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.562119</td>\n",
       "      <td>3.267666</td>\n",
       "      <td>41</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.558145</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.182322</td>\n",
       "      <td>3.825375</td>\n",
       "      <td>70</td>\n",
       "      <td>1.658228</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.599388</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lcavol   lweight  age      lbph  svi       lcp  gleason  pgg45  \\\n",
       "1  -0.579818  2.769459   50 -1.386294    0 -1.386294        6      0   \n",
       "2  -0.994252  3.319626   58 -1.386294    0 -1.386294        6      0   \n",
       "3  -0.510826  2.691243   74 -1.386294    0 -1.386294        7     20   \n",
       "4  -1.203973  3.282789   58 -1.386294    0 -1.386294        6      0   \n",
       "5   0.751416  3.432373   62 -1.386294    0 -1.386294        6      0   \n",
       "6  -1.049822  3.228826   50 -1.386294    0 -1.386294        6      0   \n",
       "7   0.737164  3.473518   64  0.615186    0 -1.386294        6      0   \n",
       "8   0.693147  3.539509   58  1.536867    0 -1.386294        6      0   \n",
       "9  -0.776529  3.539509   47 -1.386294    0 -1.386294        6      0   \n",
       "10  0.223144  3.244544   63 -1.386294    0 -1.386294        6      0   \n",
       "11  0.254642  3.604138   65 -1.386294    0 -1.386294        6      0   \n",
       "12 -1.347074  3.598681   63  1.266948    0 -1.386294        6      0   \n",
       "13  1.613430  3.022861   63 -1.386294    0 -0.597837        7     30   \n",
       "14  1.477049  2.998229   67 -1.386294    0 -1.386294        7      5   \n",
       "15  1.205971  3.442019   57 -1.386294    0 -0.430783        7      5   \n",
       "16  1.541159  3.061052   66 -1.386294    0 -1.386294        6      0   \n",
       "17 -0.415515  3.516013   70  1.244155    0 -0.597837        7     30   \n",
       "18  2.288486  3.649359   66 -1.386294    0  0.371564        6      0   \n",
       "19 -0.562119  3.267666   41 -1.386294    0 -1.386294        6      0   \n",
       "20  0.182322  3.825375   70  1.658228    0 -1.386294        6      0   \n",
       "\n",
       "        lpsa train  \n",
       "1  -0.430783     T  \n",
       "2  -0.162519     T  \n",
       "3  -0.162519     T  \n",
       "4  -0.162519     T  \n",
       "5   0.371564     T  \n",
       "6   0.765468     T  \n",
       "7   0.765468     F  \n",
       "8   0.854415     T  \n",
       "9   1.047319     F  \n",
       "10  1.047319     F  \n",
       "11  1.266948     T  \n",
       "12  1.266948     T  \n",
       "13  1.266948     T  \n",
       "14  1.348073     T  \n",
       "15  1.398717     F  \n",
       "16  1.446919     T  \n",
       "17  1.470176     T  \n",
       "18  1.492904     T  \n",
       "19  1.558145     T  \n",
       "20  1.599388     T  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prostate_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "\n",
    "feature_names = ['lcavol', 'lweight',   'age',  'lbph', 'svi', 'lcp',   \n",
    "                 'gleason', 'pgg45']\n",
    "\n",
    "# Normalize features\n",
    "scalar = preprocessing.StandardScaler()\n",
    "prostate_df[feature_names] = pd.DataFrame(\n",
    "    scalar.fit_transform(\n",
    "        prostate_df[feature_names].astype('float64')))\n",
    "\n",
    "# select training set\n",
    "prostate_df_train = prostate_df[prostate_df.train == 'T'] \n",
    "\n",
    "# Select features and labels \n",
    "features = prostate_df_train[feature_names]\n",
    "labels =  prostate_df_train[['lpsa']]\n",
    "\n",
    "# Create tensors\n",
    "feat = tf.constant(features.values, dtype=tf.float64)\n",
    "lab = tf.constant(labels.values, dtype=tf.float64)\n",
    "\n",
    "dtype = feat.dtype\n",
    "\n",
    "regularization = 0 # regularization parameter\n",
    "dim = 8 # number of features\n",
    "\n",
    "# We pick a random starting point for the search\n",
    "start = np.random.randn(dim + 1)\n",
    "\n",
    "def regression_loss(params):\n",
    "  \"\"\"Compute loss for linear regression model with L1 penalty\n",
    "\n",
    "  Args:\n",
    "    params: A real tensor of shape [dim + 1]. The zeroth component\n",
    "      is the intercept term and the rest of the components are the\n",
    "      beta coefficients.\n",
    "\n",
    "  Returns:\n",
    "    The mean square error loss including L1 penalty.\n",
    "  \"\"\"\n",
    "  params = tf.squeeze(params)\n",
    "  intercept, beta  = params[0], params[1:]\n",
    "  pred = tf.matmul(feat, tf.expand_dims(beta, axis=-1)) + intercept\n",
    "  mse_loss = tf.reduce_sum(\n",
    "      tf.cast(\n",
    "        tf.losses.mean_squared_error(y_true=lab, y_pred=pred), tf.float64))\n",
    "  l1_penalty = regularization * tf.reduce_sum(tf.abs(beta))\n",
    "  total_loss = mse_loss + l1_penalty\n",
    "  return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation took: 0.035996 seconds\n",
      "L-BFGS Results\n",
      "Converged: True\n",
      "Intercept: Fitted (0.6910553802077484)\n",
      "Beta:      Fitted [-0.02238219  0.06548258 -0.06823577 -0.06574284  0.24958369]\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def l1_regression_with_lbfgs():\n",
    "  return tfp.optimizer.lbfgs_minimize(\n",
    "    make_val_and_grad_fn(regression_loss),\n",
    "    initial_position=tf.constant(start),\n",
    "    tolerance=1e-8)\n",
    "\n",
    "results = run(l1_regression_with_lbfgs)\n",
    "minimum = results.position\n",
    "fitted_intercept = minimum[0]\n",
    "fitted_beta = minimum[1:]\n",
    "\n",
    "print('L-BFGS Results')\n",
    "print('Converged:', results.converged)\n",
    "print('Intercept: Fitted ({})'.format(fitted_intercept))\n",
    "print('Beta:      Fitted {}'.format(fitted_beta))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving problems using the downhill simplex method\n",
    "\n",
    "The downhill simplex method is one of the most popular derivative-free minimization methods. This optimizer does not use gradient information, does not assume differentiability of the objective function, and is therefore suitable for non-smooth objective functions, e.g., optimization problems that use L1 penalties.\n",
    "\n",
    "For the n-dimensional optimization problem, the optimizer maintains a set of n+1 candidate solutions that span non-degenerate simplexes. The optimizer continuously modifies the simplexes based on a set of changes (reflections, expansions, contractions, and tightenings) using the function values of each vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation took: 0.442060 seconds\n",
      "Nelder Mead Results\n",
      "Converged: True\n",
      "Intercept: Fitted (0.6910553769315656)\n",
      "Beta:      Fitted [-0.02238214  0.06548257 -0.0682358  -0.06574288  0.24958364]\n"
     ]
    }
   ],
   "source": [
    "# Nelder mead expects an initial_vertex of shape [n + 1, 1].\n",
    "initial_vertex = tf.expand_dims(tf.constant(start, dtype=dtype), axis=-1)\n",
    "\n",
    "@tf.function\n",
    "def l1_regression_with_nelder_mead():\n",
    "  return tfp.optimizer.nelder_mead_minimize(\n",
    "      regression_loss,\n",
    "      initial_vertex=initial_vertex,\n",
    "      func_tolerance=1e-10,\n",
    "      position_tolerance=1e-10)\n",
    "\n",
    "results = run(l1_regression_with_nelder_mead)\n",
    "minimum = results.position.reshape([-1])\n",
    "fitted_intercept = minimum[0]\n",
    "fitted_beta = minimum[1:]\n",
    "\n",
    "print('Nelder Mead Results')\n",
    "print('Converged:', results.converged)\n",
    "print('Intercept: Fitted ({})'.format(fitted_intercept))\n",
    "print('Beta:      Fitted {}'.format(fitted_beta))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression using L2 penalty\n",
    "In this example, we create a composite dataset for classification and use the L-BFGS optimizer to fit the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation took: 0.138834 seconds\n",
      "Converged: True\n",
      "Intercept: Fitted (1.4111415084244365), Actual (1.3934058329729904)\n",
      "Beta:\n",
      "\tFitted [-0.18016612  0.53121578 -0.56420632 -0.5336374   2.00499675],\n",
      "\tActual [-0.20470766  0.47894334 -0.51943872 -0.5557303   1.96578057]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(12345)\n",
    "\n",
    "dim = 5  # The number of features\n",
    "n_obs = 10000  # The number of observations\n",
    "\n",
    "betas = np.random.randn(dim)  # The true beta\n",
    "intercept = np.random.randn()  # The true intercept\n",
    "\n",
    "features = np.random.randn(n_obs, dim)  # The feature matrix\n",
    "probs = sp.special.expit(\n",
    "    np.matmul(features, np.expand_dims(betas, -1)) + intercept)\n",
    "\n",
    "labels = sp.stats.bernoulli.rvs(probs)  # The true labels\n",
    "\n",
    "regularization = 0.8\n",
    "feat = tf.constant(features)\n",
    "lab = tf.constant(labels, dtype=feat.dtype)\n",
    "\n",
    "@make_val_and_grad_fn\n",
    "def negative_log_likelihood(params):\n",
    "  \"\"\"Negative log likelihood for logistic model with L2 penalty\n",
    "\n",
    "  Args:\n",
    "    params: A real tensor of shape [dim + 1]. The zeroth component\n",
    "      is the intercept term and the rest of the components are the\n",
    "      beta coefficients.\n",
    "\n",
    "  Returns:\n",
    "    The negative log likelihood plus the penalty term. \n",
    "  \"\"\"\n",
    "  intercept, beta  = params[0], params[1:]\n",
    "  logit = tf.matmul(feat, tf.expand_dims(beta, -1)) + intercept\n",
    "  log_likelihood = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "      labels=lab, logits=logit))\n",
    "  l2_penalty = regularization * tf.reduce_sum(beta ** 2)\n",
    "  total_loss = log_likelihood + l2_penalty\n",
    "  return total_loss\n",
    "\n",
    "start = np.random.randn(dim + 1)\n",
    "\n",
    "@tf.function\n",
    "def l2_regression_with_lbfgs():\n",
    "  return tfp.optimizer.lbfgs_minimize(\n",
    "      negative_log_likelihood,\n",
    "      initial_position=tf.constant(start),\n",
    "      tolerance=1e-8)\n",
    "\n",
    "results = run(l2_regression_with_lbfgs)\n",
    "minimum = results.position\n",
    "fitted_intercept = minimum[0]\n",
    "fitted_beta = minimum[1:]\n",
    "\n",
    "print('Converged:', results.converged)\n",
    "print('Intercept: Fitted ({}), Actual ({})'.format(fitted_intercept, intercept))\n",
    "print('Beta:\\n\\tFitted {},\\n\\tActual {}'.format(fitted_beta, betas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
